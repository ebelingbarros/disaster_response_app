{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kaypa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kaypa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kaypa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///Messages.db')\n",
    "df = pd.read_sql(\"SELECT * FROM Messages\", engine)\n",
    "\n",
    "X = df['message']\n",
    "Y = df.drop(['index', 'message', 'original', 'genre'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26243</th>\n",
       "      <td>26243</td>\n",
       "      <td>\"The pattern is always the same: steal, loot, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26244</th>\n",
       "      <td>26244</td>\n",
       "      <td>Sirajganj, Bangladesh. When the rain-swollen J...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26245</th>\n",
       "      <td>26245</td>\n",
       "      <td>Damage to the Nacala rail corridor by the rain...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26246</th>\n",
       "      <td>26246</td>\n",
       "      <td>Thus, an armed drone strike in Pakistan, a Nig...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26247</th>\n",
       "      <td>26247</td>\n",
       "      <td>This year's 16 Days theme, \"From Peace in the ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26248 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                            message  \\\n",
       "0          0  Weather update - a cold front from Cuba that c...   \n",
       "1          1            Is the Hurricane over or is it not over   \n",
       "2          2                    Looking for someone but no name   \n",
       "3          3  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4          4  says: west side of Haiti, rest of the country ...   \n",
       "...      ...                                                ...   \n",
       "26243  26243  \"The pattern is always the same: steal, loot, ...   \n",
       "26244  26244  Sirajganj, Bangladesh. When the rain-swollen J...   \n",
       "26245  26245  Damage to the Nacala rail corridor by the rain...   \n",
       "26246  26246  Thus, an armed drone strike in Pakistan, a Nig...   \n",
       "26247  26247  This year's 16 Days theme, \"From Peace in the ...   \n",
       "\n",
       "                                                original   genre  related  \\\n",
       "0      Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "1                     Cyclone nan fini osinon li pa fini  direct        1   \n",
       "2      Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n",
       "3      UN reports Leogane 80-90 destroyed. Only Hospi...  direct        1   \n",
       "4      facade ouest d Haiti et le reste du pays aujou...  direct        1   \n",
       "...                                                  ...     ...      ...   \n",
       "26243                                               None    news        0   \n",
       "26244                                               None    news        0   \n",
       "26245                                               None    news        1   \n",
       "26246                                               None    news        1   \n",
       "26247                                               None    news        1   \n",
       "\n",
       "       request  offer  aid_related  medical_help  medical_products  ...  \\\n",
       "0            0      0            0             0                 0  ...   \n",
       "1            0      0            1             0                 0  ...   \n",
       "2            0      0            0             0                 0  ...   \n",
       "3            1      0            1             0                 1  ...   \n",
       "4            0      0            0             0                 0  ...   \n",
       "...        ...    ...          ...           ...               ...  ...   \n",
       "26243        0      0            0             0                 0  ...   \n",
       "26244        0      0            0             0                 0  ...   \n",
       "26245        0      0            0             0                 0  ...   \n",
       "26246        0      0            1             0                 0  ...   \n",
       "26247        0      0            0             0                 0  ...   \n",
       "\n",
       "       aid_centers  other_infrastructure  weather_related  floods  storm  \\\n",
       "0                0                     0                0       0      0   \n",
       "1                0                     0                1       0      1   \n",
       "2                0                     0                0       0      0   \n",
       "3                0                     0                0       0      0   \n",
       "4                0                     0                0       0      0   \n",
       "...            ...                   ...              ...     ...    ...   \n",
       "26243            0                     0                0       0      0   \n",
       "26244            0                     0                0       0      0   \n",
       "26245            0                     0                0       0      0   \n",
       "26246            0                     0                0       0      0   \n",
       "26247            0                     0                0       0      0   \n",
       "\n",
       "       fire  earthquake  cold  other_weather  direct_report  \n",
       "0         0           0     0              0              0  \n",
       "1         0           0     0              0              0  \n",
       "2         0           0     0              0              0  \n",
       "3         0           0     0              0              0  \n",
       "4         0           0     0              0              0  \n",
       "...     ...         ...   ...            ...            ...  \n",
       "26243     0           0     0              0              0  \n",
       "26244     0           0     0              0              0  \n",
       "26245     0           0     0              0              0  \n",
       "26246     0           0     0              0              0  \n",
       "26247     0           0     0              0              0  \n",
       "\n",
       "[26248 rows x 40 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Normalize, tokenize and stem text string\n",
    "    \n",
    "    Args:\n",
    "    text: string. String containing message for processing\n",
    "       \n",
    "    Returns:\n",
    "    stemmed: list of strings. List containing normalized and stemmed word tokens\n",
    "    \"\"\"\n",
    "    # Convert text to lowercase and remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    # Tokenize words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Stem word tokens and remove stop words\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    stemmed = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "- You'll find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 CountVectorizer(tokenizer=<function tokenize at 0x000001F577E88820>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('clf',\n",
       "                 MultiOutputClassifier(estimator=RandomForestClassifier()))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 1)\n",
    "\n",
    "np.random.seed(17)\n",
    "pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall on both the training set and the test set. You can use sklearn's `classification_report` function here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_metrics(actual, predicted, col_names):\n",
    "    \"\"\"Calculate evaluation metrics for ML model\n",
    "    \n",
    "    Args:\n",
    "    actual: array. Array containing actual labels.\n",
    "    predicted: array. Array containing predicted labels.\n",
    "    col_names: list of strings. List containing names for each of the predicted fields.\n",
    "       \n",
    "    Returns:\n",
    "    metrics_df: dataframe. Dataframe containing the accuracy, precision, recall \n",
    "    and f1 score for a given set of actual and predicted labels.\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    \n",
    "    # Calculate evaluation metrics for each set of labels\n",
    "    for i in range(len(col_names)):\n",
    "        accuracy = accuracy_score(actual[:, i], predicted[:, i])\n",
    "        precision = precision_score(actual[:, i], predicted[:, i], average='weighted')\n",
    "        recall = recall_score(actual[:, i], predicted[:, i], average='weighted')\n",
    "        f1 = f1_score(actual[:, i], predicted[:, i], average='weighted')\n",
    "        \n",
    "        metrics.append([accuracy, precision, recall, f1])\n",
    "    \n",
    "    # Create dataframe containing metrics\n",
    "    metrics = np.array(metrics)\n",
    "    metrics_df = pd.DataFrame(data = metrics, index = col_names, columns = ['Accuracy', 'Precision', 'Recall', 'F1'])\n",
    "      \n",
    "    return metrics_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.995327   0.995324  0.995327  0.995319\n",
      "request                 0.995987   0.995981  0.995987  0.995979\n",
      "offer                   0.999898   0.999898  0.999898  0.999898\n",
      "aid_related             0.993904   0.993906  0.993904  0.993903\n",
      "medical_help            0.998222   0.998223  0.998222  0.998214\n",
      "medical_products        0.998019   0.998015  0.998019  0.998004\n",
      "search_and_rescue       0.999746   0.999746  0.999746  0.999745\n",
      "security                0.999644   0.999645  0.999644  0.999643\n",
      "military                0.999644   0.999645  0.999644  0.999644\n",
      "child_alone             1.000000   1.000000  1.000000  1.000000\n",
      "water                   0.998324   0.998324  0.998324  0.998315\n",
      "food                    0.997613   0.997612  0.997613  0.997604\n",
      "shelter                 0.997359   0.997361  0.997359  0.997343\n",
      "clothing                0.999594   0.999593  0.999594  0.999592\n",
      "money                   0.999441   0.999442  0.999441  0.999438\n",
      "missing_people          0.999898   0.999898  0.999898  0.999898\n",
      "refugees                0.999390   0.999391  0.999390  0.999388\n",
      "death                   0.999136   0.999137  0.999136  0.999133\n",
      "other_aid               0.997206   0.997207  0.997206  0.997196\n",
      "infrastructure_related  0.999289   0.999289  0.999289  0.999287\n",
      "transport               0.999238   0.999239  0.999238  0.999235\n",
      "buildings               0.998933   0.998933  0.998933  0.998928\n",
      "electricity             0.999492   0.999492  0.999492  0.999489\n",
      "tools                   1.000000   1.000000  1.000000  1.000000\n",
      "hospitals               0.999898   0.999898  0.999898  0.999898\n",
      "shops                   0.999898   0.999898  0.999898  0.999898\n",
      "aid_centers             0.999746   0.999746  0.999746  0.999745\n",
      "other_infrastructure    0.999238   0.999239  0.999238  0.999235\n",
      "weather_related         0.995936   0.995937  0.995936  0.995932\n",
      "floods                  0.998933   0.998933  0.998933  0.998931\n",
      "storm                   0.998476   0.998474  0.998476  0.998474\n",
      "fire                    0.999848   0.999848  0.999848  0.999847\n",
      "earthquake              0.997867   0.997864  0.997867  0.997859\n",
      "cold                    0.999136   0.999137  0.999136  0.999127\n",
      "other_weather           0.998832   0.998833  0.998832  0.998826\n",
      "direct_report           0.995733   0.995728  0.995733  0.995725\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for training set\n",
    "Y_train_pred = pipeline.predict(X_train)\n",
    "col_names = list(Y.columns.values)\n",
    "\n",
    "print(get_eval_metrics(np.array(Y_train), Y_train_pred, col_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.745809   0.654134  0.745809  0.666957\n",
      "request                 0.826120   0.768725  0.826120  0.766090\n",
      "offer                   0.996038   0.992394  0.996038  0.994213\n",
      "aid_related             0.571167   0.539889  0.571167  0.530296\n",
      "medical_help            0.922280   0.863030  0.922280  0.889645\n",
      "medical_products        0.947120   0.910747  0.947120  0.923450\n",
      "search_and_rescue       0.974855   0.950343  0.974855  0.962443\n",
      "security                0.981865   0.964059  0.981865  0.972881\n",
      "military                0.969521   0.939972  0.969521  0.954518\n",
      "child_alone             1.000000   1.000000  1.000000  1.000000\n",
      "water                   0.933252   0.886075  0.933252  0.905095\n",
      "food                    0.885706   0.808963  0.885706  0.837570\n",
      "shelter                 0.906126   0.836587  0.906126  0.868424\n",
      "clothing                0.983237   0.975281  0.983237  0.975224\n",
      "money                   0.976989   0.954801  0.976989  0.965768\n",
      "missing_people          0.987656   0.975465  0.987656  0.981523\n",
      "refugees                0.965102   0.932000  0.965102  0.948262\n",
      "death                   0.954739   0.924094  0.954739  0.933815\n",
      "other_aid               0.864371   0.780701  0.864371  0.810565\n",
      "infrastructure_related  0.934014   0.878459  0.934014  0.905385\n",
      "transport               0.952758   0.914754  0.952758  0.931190\n",
      "buildings               0.945748   0.902207  0.945748  0.922329\n",
      "electricity             0.979122   0.958976  0.979122  0.968944\n",
      "tools                   0.992838   0.985726  0.992838  0.989269\n",
      "hospitals               0.991314   0.982703  0.991314  0.986989\n",
      "shops                   0.995581   0.991181  0.995581  0.993376\n",
      "aid_centers             0.987047   0.974560  0.987047  0.980764\n",
      "other_infrastructure    0.957940   0.918792  0.957940  0.937958\n",
      "weather_related         0.734227   0.698965  0.734227  0.676997\n",
      "floods                  0.915422   0.858330  0.915422  0.880598\n",
      "storm                   0.907193   0.867765  0.907193  0.871228\n",
      "fire                    0.989942   0.979985  0.989942  0.984939\n",
      "earthquake              0.915422   0.899457  0.915422  0.890189\n",
      "cold                    0.980799   0.962262  0.980799  0.971442\n",
      "other_weather           0.948948   0.901914  0.948948  0.924833\n",
      "direct_report           0.802804   0.735651  0.802804  0.734238\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for test set\n",
    "Y_test_pred = pipeline.predict(X_test)\n",
    "\n",
    "eval_metrics0 = get_eval_metrics(np.array(Y_test), Y_test_pred, col_names)\n",
    "print(eval_metrics0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the vast majority of the categories, the indicators Accuracy, Precision, Recall and F1 are very good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define performance metric for use in grid search scoring object\n",
    "def performance_metric(y_true, y_pred):\n",
    "    \"\"\"Calculate median F1 score for all of the output classifiers\n",
    "    \n",
    "    Args:\n",
    "    y_true: array. Array containing actual labels.\n",
    "    y_pred: array. Array containing predicted labels.\n",
    "        \n",
    "    Returns:\n",
    "    score: float. Median F1 score for all of the output classifiers\n",
    "    \"\"\"\n",
    "    f1_list = []\n",
    "    for i in range(np.shape(y_pred)[1]):\n",
    "        f1 = f1_score(np.array(y_true)[:, i], y_pred[:, i])\n",
    "        f1_list.append(f1)\n",
    "        \n",
    "    score = np.median(f1_list)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is very handy to choose how to improve the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'vect', 'tfidf', 'clf', 'vect__analyzer', 'vect__binary', 'vect__decode_error', 'vect__dtype', 'vect__encoding', 'vect__input', 'vect__lowercase', 'vect__max_df', 'vect__max_features', 'vect__min_df', 'vect__ngram_range', 'vect__preprocessor', 'vect__stop_words', 'vect__strip_accents', 'vect__token_pattern', 'vect__tokenizer', 'vect__vocabulary', 'tfidf__norm', 'tfidf__smooth_idf', 'tfidf__sublinear_tf', 'tfidf__use_idf', 'clf__estimator__bootstrap', 'clf__estimator__ccp_alpha', 'clf__estimator__class_weight', 'clf__estimator__criterion', 'clf__estimator__max_depth', 'clf__estimator__max_features', 'clf__estimator__max_leaf_nodes', 'clf__estimator__max_samples', 'clf__estimator__min_impurity_decrease', 'clf__estimator__min_impurity_split', 'clf__estimator__min_samples_leaf', 'clf__estimator__min_samples_split', 'clf__estimator__min_weight_fraction_leaf', 'clf__estimator__n_estimators', 'clf__estimator__n_jobs', 'clf__estimator__oob_score', 'clf__estimator__random_state', 'clf__estimator__verbose', 'clf__estimator__warm_start', 'clf__estimator', 'clf__n_jobs'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use 'clf__estimator__class_weight': ['balanced'] to obtain more diversified results. In other words, to avoid possible issues of unbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=2, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=5; total time=  24.9s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=2, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=5; total time=  21.2s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=2, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=5; total time=  20.5s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=2, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=5; total time=  21.1s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=2, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=5; total time=  21.9s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=2, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10; total time=  22.0s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=2, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10; total time=  21.0s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=2, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10; total time=  21.2s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=2, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10; total time=  21.0s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=2, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10; total time=  20.7s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=2, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=5; total time=  19.4s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=2, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=5; total time=  20.2s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=2, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=5; total time=  20.1s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=2, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=5; total time=  19.7s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=2, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=5; total time=  19.9s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=2, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10; total time=  20.6s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=2, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10; total time=  20.6s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=2, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10; total time=  20.5s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=2, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10; total time=  22.3s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=2, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10; total time=  21.1s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=5; total time=  20.0s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=5; total time=  19.7s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=5; total time=  19.9s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=5; total time=  21.3s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=5; total time=  19.6s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10; total time=  21.5s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10; total time=  21.4s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10; total time=  20.7s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10; total time=  21.4s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=5, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10; total time=  22.5s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=5; total time=  19.9s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=5; total time=  20.8s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=5; total time=  20.1s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=5; total time=  20.3s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=5; total time=  20.6s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10; total time=  20.9s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10; total time=  21.5s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10; total time=  21.4s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10; total time=  20.9s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=5, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10; total time=  21.5s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=5; total time=  21.2s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=5; total time=  20.3s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=5; total time=  20.8s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=5; total time=  20.7s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=5; total time=  21.0s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10; total time=  21.5s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10; total time=  21.5s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10; total time=  21.4s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10; total time=  21.2s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=10, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10; total time=  21.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=10, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=5; total time=  20.2s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=10, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=5; total time=  20.4s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=10, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=5; total time=  19.7s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=10, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=5; total time=  20.2s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=10, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=5; total time=  20.4s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=10, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10; total time=  21.3s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=10, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10; total time=  21.0s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=10, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10; total time=  21.4s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=10, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10; total time=  20.9s\n",
      "[CV] END clf__estimator__class_weight=balanced, clf__estimator__max_depth=10, clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10; total time=  21.1s\n"
     ]
    }
   ],
   "source": [
    "# Create grid search object\n",
    "\n",
    "parameters = {'clf__estimator__max_depth':[2, 5, 10],\n",
    "              'clf__estimator__n_estimators':[5, 10], \n",
    "              'clf__estimator__min_samples_split':[2, 5],\n",
    "             'clf__estimator__class_weight': ['balanced'] }\n",
    "\n",
    "scorer = make_scorer(performance_metric)\n",
    "cv = GridSearchCV(pipeline, param_grid = parameters, scoring = scorer, verbose = 2)\n",
    "\n",
    "# Find best parameters\n",
    "np.random.seed(81)\n",
    "tuned_model = cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([15.960957  , 16.74645953, 17.18199406, 17.00588093, 15.78674955,\n",
       "        15.88934331]),\n",
       " 'std_fit_time': array([0.23333287, 0.77601532, 0.83868129, 1.83795215, 0.19691544,\n",
       "        0.33368533]),\n",
       " 'mean_score_time': array([3.92424855, 4.05096126, 4.10611706, 3.84239254, 3.85023494,\n",
       "        3.8075943 ]),\n",
       " 'std_score_time': array([0.05927206, 0.22392025, 0.10671318, 0.09237727, 0.08514312,\n",
       "        0.17822861]),\n",
       " 'param_clf__estimator__max_depth': masked_array(data=[2, 2, 5, 5, 10, 10],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_clf__estimator__min_samples_split': masked_array(data=[2, 5, 2, 5, 2, 5],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_clf__estimator__n_estimators': masked_array(data=[5, 5, 5, 5, 5, 5],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'clf__estimator__max_depth': 2,\n",
       "   'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 5},\n",
       "  {'clf__estimator__max_depth': 2,\n",
       "   'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 5},\n",
       "  {'clf__estimator__max_depth': 5,\n",
       "   'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 5},\n",
       "  {'clf__estimator__max_depth': 5,\n",
       "   'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 5},\n",
       "  {'clf__estimator__max_depth': 10,\n",
       "   'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 5},\n",
       "  {'clf__estimator__max_depth': 10,\n",
       "   'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 5}],\n",
       " 'split0_test_score': array([nan, nan, nan, nan, nan, nan]),\n",
       " 'split1_test_score': array([nan, nan, nan, nan, nan, nan]),\n",
       " 'split2_test_score': array([nan, nan, nan, nan, nan, nan]),\n",
       " 'split3_test_score': array([nan, nan, nan, nan, nan, nan]),\n",
       " 'split4_test_score': array([nan, nan, nan, nan, nan, nan]),\n",
       " 'mean_test_score': array([nan, nan, nan, nan, nan, nan]),\n",
       " 'std_test_score': array([nan, nan, nan, nan, nan, nan]),\n",
       " 'rank_test_score': array([1, 2, 3, 4, 5, 6])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get results of grid search\n",
    "tuned_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__max_depth': 2,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__n_estimators': 5}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters for best mean test score\n",
    "tuned_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.758915   0.575952  0.758915  0.654895\n",
      "request                 0.828254   0.686004  0.828254  0.750447\n",
      "offer                   0.996190   0.992395  0.996190  0.994289\n",
      "aid_related             0.590674   0.531100  0.590674  0.441091\n",
      "medical_help            0.926547   0.858489  0.926547  0.891220\n",
      "medical_products        0.948339   0.899347  0.948339  0.923193\n",
      "search_and_rescue       0.974855   0.950343  0.974855  0.962443\n",
      "security                0.981865   0.964059  0.981865  0.972881\n",
      "military                0.969521   0.939972  0.969521  0.954518\n",
      "child_alone             1.000000   1.000000  1.000000  1.000000\n",
      "water                   0.936300   0.876658  0.936300  0.905498\n",
      "food                    0.889820   0.791780  0.889820  0.837942\n",
      "shelter                 0.912831   0.833261  0.912831  0.871233\n",
      "clothing                0.983237   0.966755  0.983237  0.974926\n",
      "money                   0.977141   0.954805  0.977141  0.965844\n",
      "missing_people          0.987656   0.975465  0.987656  0.981523\n",
      "refugees                0.965407   0.932010  0.965407  0.948415\n",
      "death                   0.955349   0.912692  0.955349  0.933533\n",
      "other_aid               0.870923   0.758508  0.870923  0.810838\n",
      "infrastructure_related  0.937367   0.878656  0.937367  0.907062\n",
      "transport               0.953977   0.910073  0.953977  0.931508\n",
      "buildings               0.948491   0.899636  0.948491  0.923418\n",
      "electricity             0.979275   0.958979  0.979275  0.969020\n",
      "tools                   0.992838   0.985726  0.992838  0.989269\n",
      "hospitals               0.991314   0.982703  0.991314  0.986989\n",
      "shops                   0.995581   0.991181  0.995581  0.993376\n",
      "aid_centers             0.987199   0.974562  0.987199  0.980840\n",
      "other_infrastructure    0.958549   0.918817  0.958549  0.938262\n",
      "weather_related         0.723103   0.522878  0.723103  0.606902\n",
      "floods                  0.919537   0.845548  0.919537  0.880992\n",
      "storm                   0.908412   0.825212  0.908412  0.864816\n",
      "fire                    0.989942   0.979985  0.989942  0.984939\n",
      "earthquake              0.907193   0.822999  0.907193  0.863047\n",
      "cold                    0.980951   0.962265  0.980951  0.971518\n",
      "other_weather           0.949710   0.901950  0.949710  0.925214\n",
      "direct_report           0.806461   0.650380  0.806461  0.720060\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for test set\n",
    "tuned_pred_test = tuned_model.predict(X_test)\n",
    "\n",
    "eval_metrics1 = get_eval_metrics(np.array(Y_test), tuned_pred_test, col_names)\n",
    "\n",
    "print(eval_metrics1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.925641</td>\n",
       "      <td>0.890804</td>\n",
       "      <td>0.925641</td>\n",
       "      <td>0.901039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.090547</td>\n",
       "      <td>0.105334</td>\n",
       "      <td>0.090547</td>\n",
       "      <td>0.107376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.571167</td>\n",
       "      <td>0.539889</td>\n",
       "      <td>0.571167</td>\n",
       "      <td>0.530296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.913365</td>\n",
       "      <td>0.861855</td>\n",
       "      <td>0.913365</td>\n",
       "      <td>0.878255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.953749</td>\n",
       "      <td>0.916773</td>\n",
       "      <td>0.953749</td>\n",
       "      <td>0.932502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.982208</td>\n",
       "      <td>0.966685</td>\n",
       "      <td>0.982208</td>\n",
       "      <td>0.973467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Precision     Recall         F1\n",
       "count  36.000000  36.000000  36.000000  36.000000\n",
       "mean    0.925641   0.890804   0.925641   0.901039\n",
       "std     0.090547   0.105334   0.090547   0.107376\n",
       "min     0.571167   0.539889   0.571167   0.530296\n",
       "25%     0.913365   0.861855   0.913365   0.878255\n",
       "50%     0.953749   0.916773   0.953749   0.932502\n",
       "75%     0.982208   0.966685   0.982208   0.973467\n",
       "max     1.000000   1.000000   1.000000   1.000000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get summary stats for first model\n",
    "eval_metrics0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.927326</td>\n",
       "      <td>0.872532</td>\n",
       "      <td>0.927326</td>\n",
       "      <td>0.894777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.088051</td>\n",
       "      <td>0.131044</td>\n",
       "      <td>0.088051</td>\n",
       "      <td>0.122974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.590674</td>\n",
       "      <td>0.522878</td>\n",
       "      <td>0.590674</td>\n",
       "      <td>0.441091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.911727</td>\n",
       "      <td>0.831249</td>\n",
       "      <td>0.911727</td>\n",
       "      <td>0.869629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.954663</td>\n",
       "      <td>0.911382</td>\n",
       "      <td>0.954663</td>\n",
       "      <td>0.932521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.982208</td>\n",
       "      <td>0.964733</td>\n",
       "      <td>0.982208</td>\n",
       "      <td>0.973392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Precision     Recall         F1\n",
       "count  36.000000  36.000000  36.000000  36.000000\n",
       "mean    0.927326   0.872532   0.927326   0.894777\n",
       "std     0.088051   0.131044   0.088051   0.122974\n",
       "min     0.590674   0.522878   0.590674   0.441091\n",
       "25%     0.911727   0.831249   0.911727   0.869629\n",
       "50%     0.954663   0.911382   0.954663   0.932521\n",
       "75%     0.982208   0.964733   0.982208   0.973392\n",
       "max     1.000000   1.000000   1.000000   1.000000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get summary stats for tuned model\n",
    "eval_metrics1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the following code is very important to know how to build new gridseach parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'vect', 'tfidf', 'clf', 'vect__analyzer', 'vect__binary', 'vect__decode_error', 'vect__dtype', 'vect__encoding', 'vect__input', 'vect__lowercase', 'vect__max_df', 'vect__max_features', 'vect__min_df', 'vect__ngram_range', 'vect__preprocessor', 'vect__stop_words', 'vect__strip_accents', 'vect__token_pattern', 'vect__tokenizer', 'vect__vocabulary', 'tfidf__norm', 'tfidf__smooth_idf', 'tfidf__sublinear_tf', 'tfidf__use_idf', 'clf__estimator__algorithm', 'clf__estimator__leaf_size', 'clf__estimator__metric', 'clf__estimator__metric_params', 'clf__estimator__n_jobs', 'clf__estimator__n_neighbors', 'clf__estimator__p', 'clf__estimator__weights', 'clf__estimator', 'clf__n_jobs'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline2.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV 1/5; 1/8] START clf__estimator__leaf_size=2, clf__estimator__n_neighbors=1, clf__estimator__p=1\n",
      "[CV 1/5; 1/8] END clf__estimator__leaf_size=2, clf__estimator__n_neighbors=1, clf__estimator__p=1; total time= 1.5min\n",
      "[CV 2/5; 1/8] START clf__estimator__leaf_size=2, clf__estimator__n_neighbors=1, clf__estimator__p=1\n",
      "[CV 2/5; 1/8] END clf__estimator__leaf_size=2, clf__estimator__n_neighbors=1, clf__estimator__p=1; total time= 1.6min\n",
      "[CV 3/5; 1/8] START clf__estimator__leaf_size=2, clf__estimator__n_neighbors=1, clf__estimator__p=1\n",
      "[CV 3/5; 1/8] END clf__estimator__leaf_size=2, clf__estimator__n_neighbors=1, clf__estimator__p=1; total time= 1.9min\n",
      "[CV 4/5; 1/8] START clf__estimator__leaf_size=2, clf__estimator__n_neighbors=1, clf__estimator__p=1\n",
      "[CV 4/5; 1/8] END clf__estimator__leaf_size=2, clf__estimator__n_neighbors=1, clf__estimator__p=1; total time= 1.7min\n",
      "[CV 5/5; 1/8] START clf__estimator__leaf_size=2, clf__estimator__n_neighbors=1, clf__estimator__p=1\n",
      "[CV 5/5; 1/8] END clf__estimator__leaf_size=2, clf__estimator__n_neighbors=1, clf__estimator__p=1; total time= 1.7min\n",
      "[CV 1/5; 2/8] START clf__estimator__leaf_size=2, clf__estimator__n_neighbors=1, clf__estimator__p=5\n",
      "[CV 1/5; 2/8] END clf__estimator__leaf_size=2, clf__estimator__n_neighbors=1, clf__estimator__p=5; total time=  14.6s\n",
      "[CV 2/5; 2/8] START clf__estimator__leaf_size=2, clf__estimator__n_neighbors=1, clf__estimator__p=5\n",
      "[CV 2/5; 2/8] END clf__estimator__leaf_size=2, clf__estimator__n_neighbors=1, clf__estimator__p=5; total time=  15.3s\n",
      "[CV 3/5; 2/8] START clf__estimator__leaf_size=2, clf__estimator__n_neighbors=1, clf__estimator__p=5\n",
      "[CV 3/5; 2/8] END clf__estimator__leaf_size=2, clf__estimator__n_neighbors=1, clf__estimator__p=5; total time=  14.9s\n",
      "[CV 4/5; 2/8] START clf__estimator__leaf_size=2, clf__estimator__n_neighbors=1, clf__estimator__p=5\n",
      "[CV 4/5; 2/8] END clf__estimator__leaf_size=2, clf__estimator__n_neighbors=1, clf__estimator__p=5; total time=  14.2s\n",
      "[CV 5/5; 2/8] START clf__estimator__leaf_size=2, clf__estimator__n_neighbors=1, clf__estimator__p=5\n",
      "[CV 5/5; 2/8] END clf__estimator__leaf_size=2, clf__estimator__n_neighbors=1, clf__estimator__p=5; total time=  14.1s\n",
      "[CV 1/5; 3/8] START clf__estimator__leaf_size=2, clf__estimator__n_neighbors=5, clf__estimator__p=1\n",
      "[CV 1/5; 3/8] END clf__estimator__leaf_size=2, clf__estimator__n_neighbors=5, clf__estimator__p=1; total time= 1.9min\n",
      "[CV 2/5; 3/8] START clf__estimator__leaf_size=2, clf__estimator__n_neighbors=5, clf__estimator__p=1\n",
      "[CV 2/5; 3/8] END clf__estimator__leaf_size=2, clf__estimator__n_neighbors=5, clf__estimator__p=1; total time= 1.9min\n",
      "[CV 3/5; 3/8] START clf__estimator__leaf_size=2, clf__estimator__n_neighbors=5, clf__estimator__p=1\n",
      "[CV 3/5; 3/8] END clf__estimator__leaf_size=2, clf__estimator__n_neighbors=5, clf__estimator__p=1; total time= 1.9min\n",
      "[CV 4/5; 3/8] START clf__estimator__leaf_size=2, clf__estimator__n_neighbors=5, clf__estimator__p=1\n",
      "[CV 4/5; 3/8] END clf__estimator__leaf_size=2, clf__estimator__n_neighbors=5, clf__estimator__p=1; total time= 1.9min\n",
      "[CV 5/5; 3/8] START clf__estimator__leaf_size=2, clf__estimator__n_neighbors=5, clf__estimator__p=1\n",
      "[CV 5/5; 3/8] END clf__estimator__leaf_size=2, clf__estimator__n_neighbors=5, clf__estimator__p=1; total time= 2.0min\n",
      "[CV 1/5; 4/8] START clf__estimator__leaf_size=2, clf__estimator__n_neighbors=5, clf__estimator__p=5\n",
      "[CV 1/5; 4/8] END clf__estimator__leaf_size=2, clf__estimator__n_neighbors=5, clf__estimator__p=5; total time=  14.2s\n",
      "[CV 2/5; 4/8] START clf__estimator__leaf_size=2, clf__estimator__n_neighbors=5, clf__estimator__p=5\n",
      "[CV 2/5; 4/8] END clf__estimator__leaf_size=2, clf__estimator__n_neighbors=5, clf__estimator__p=5; total time=  14.2s\n",
      "[CV 3/5; 4/8] START clf__estimator__leaf_size=2, clf__estimator__n_neighbors=5, clf__estimator__p=5\n",
      "[CV 3/5; 4/8] END clf__estimator__leaf_size=2, clf__estimator__n_neighbors=5, clf__estimator__p=5; total time=  14.2s\n",
      "[CV 4/5; 4/8] START clf__estimator__leaf_size=2, clf__estimator__n_neighbors=5, clf__estimator__p=5\n",
      "[CV 4/5; 4/8] END clf__estimator__leaf_size=2, clf__estimator__n_neighbors=5, clf__estimator__p=5; total time=  14.4s\n",
      "[CV 5/5; 4/8] START clf__estimator__leaf_size=2, clf__estimator__n_neighbors=5, clf__estimator__p=5\n",
      "[CV 5/5; 4/8] END clf__estimator__leaf_size=2, clf__estimator__n_neighbors=5, clf__estimator__p=5; total time=  14.1s\n",
      "[CV 1/5; 5/8] START clf__estimator__leaf_size=5, clf__estimator__n_neighbors=1, clf__estimator__p=1\n",
      "[CV 1/5; 5/8] END clf__estimator__leaf_size=5, clf__estimator__n_neighbors=1, clf__estimator__p=1; total time= 1.7min\n",
      "[CV 2/5; 5/8] START clf__estimator__leaf_size=5, clf__estimator__n_neighbors=1, clf__estimator__p=1\n",
      "[CV 2/5; 5/8] END clf__estimator__leaf_size=5, clf__estimator__n_neighbors=1, clf__estimator__p=1; total time= 1.7min\n",
      "[CV 3/5; 5/8] START clf__estimator__leaf_size=5, clf__estimator__n_neighbors=1, clf__estimator__p=1\n",
      "[CV 3/5; 5/8] END clf__estimator__leaf_size=5, clf__estimator__n_neighbors=1, clf__estimator__p=1; total time= 1.7min\n",
      "[CV 4/5; 5/8] START clf__estimator__leaf_size=5, clf__estimator__n_neighbors=1, clf__estimator__p=1\n",
      "[CV 4/5; 5/8] END clf__estimator__leaf_size=5, clf__estimator__n_neighbors=1, clf__estimator__p=1; total time= 1.7min\n",
      "[CV 5/5; 5/8] START clf__estimator__leaf_size=5, clf__estimator__n_neighbors=1, clf__estimator__p=1\n",
      "[CV 5/5; 5/8] END clf__estimator__leaf_size=5, clf__estimator__n_neighbors=1, clf__estimator__p=1; total time= 1.7min\n",
      "[CV 1/5; 6/8] START clf__estimator__leaf_size=5, clf__estimator__n_neighbors=1, clf__estimator__p=5\n",
      "[CV 1/5; 6/8] END clf__estimator__leaf_size=5, clf__estimator__n_neighbors=1, clf__estimator__p=5; total time=  15.2s\n",
      "[CV 2/5; 6/8] START clf__estimator__leaf_size=5, clf__estimator__n_neighbors=1, clf__estimator__p=5\n",
      "[CV 2/5; 6/8] END clf__estimator__leaf_size=5, clf__estimator__n_neighbors=1, clf__estimator__p=5; total time=  15.0s\n",
      "[CV 3/5; 6/8] START clf__estimator__leaf_size=5, clf__estimator__n_neighbors=1, clf__estimator__p=5\n",
      "[CV 3/5; 6/8] END clf__estimator__leaf_size=5, clf__estimator__n_neighbors=1, clf__estimator__p=5; total time=  15.0s\n",
      "[CV 4/5; 6/8] START clf__estimator__leaf_size=5, clf__estimator__n_neighbors=1, clf__estimator__p=5\n",
      "[CV 4/5; 6/8] END clf__estimator__leaf_size=5, clf__estimator__n_neighbors=1, clf__estimator__p=5; total time=  15.1s\n",
      "[CV 5/5; 6/8] START clf__estimator__leaf_size=5, clf__estimator__n_neighbors=1, clf__estimator__p=5\n",
      "[CV 5/5; 6/8] END clf__estimator__leaf_size=5, clf__estimator__n_neighbors=1, clf__estimator__p=5; total time=  15.2s\n",
      "[CV 1/5; 7/8] START clf__estimator__leaf_size=5, clf__estimator__n_neighbors=5, clf__estimator__p=1\n",
      "[CV 1/5; 7/8] END clf__estimator__leaf_size=5, clf__estimator__n_neighbors=5, clf__estimator__p=1; total time= 2.1min\n",
      "[CV 2/5; 7/8] START clf__estimator__leaf_size=5, clf__estimator__n_neighbors=5, clf__estimator__p=1\n",
      "[CV 2/5; 7/8] END clf__estimator__leaf_size=5, clf__estimator__n_neighbors=5, clf__estimator__p=1; total time= 2.1min\n",
      "[CV 3/5; 7/8] START clf__estimator__leaf_size=5, clf__estimator__n_neighbors=5, clf__estimator__p=1\n",
      "[CV 3/5; 7/8] END clf__estimator__leaf_size=5, clf__estimator__n_neighbors=5, clf__estimator__p=1; total time=222.4min\n",
      "[CV 4/5; 7/8] START clf__estimator__leaf_size=5, clf__estimator__n_neighbors=5, clf__estimator__p=1\n",
      "[CV 4/5; 7/8] END clf__estimator__leaf_size=5, clf__estimator__n_neighbors=5, clf__estimator__p=1; total time= 2.0min\n",
      "[CV 5/5; 7/8] START clf__estimator__leaf_size=5, clf__estimator__n_neighbors=5, clf__estimator__p=1\n",
      "[CV 5/5; 7/8] END clf__estimator__leaf_size=5, clf__estimator__n_neighbors=5, clf__estimator__p=1; total time= 2.1min\n",
      "[CV 1/5; 8/8] START clf__estimator__leaf_size=5, clf__estimator__n_neighbors=5, clf__estimator__p=5\n",
      "[CV 1/5; 8/8] END clf__estimator__leaf_size=5, clf__estimator__n_neighbors=5, clf__estimator__p=5; total time=  14.5s\n",
      "[CV 2/5; 8/8] START clf__estimator__leaf_size=5, clf__estimator__n_neighbors=5, clf__estimator__p=5\n",
      "[CV 2/5; 8/8] END clf__estimator__leaf_size=5, clf__estimator__n_neighbors=5, clf__estimator__p=5; total time=  14.4s\n",
      "[CV 3/5; 8/8] START clf__estimator__leaf_size=5, clf__estimator__n_neighbors=5, clf__estimator__p=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 8/8] END clf__estimator__leaf_size=5, clf__estimator__n_neighbors=5, clf__estimator__p=5; total time=  15.0s\n",
      "[CV 4/5; 8/8] START clf__estimator__leaf_size=5, clf__estimator__n_neighbors=5, clf__estimator__p=5\n",
      "[CV 4/5; 8/8] END clf__estimator__leaf_size=5, clf__estimator__n_neighbors=5, clf__estimator__p=5; total time=  15.2s\n",
      "[CV 5/5; 8/8] START clf__estimator__leaf_size=5, clf__estimator__n_neighbors=5, clf__estimator__p=5\n",
      "[CV 5/5; 8/8] END clf__estimator__leaf_size=5, clf__estimator__n_neighbors=5, clf__estimator__p=5; total time=  14.7s\n"
     ]
    }
   ],
   "source": [
    "# Using SVM instead of Random Forest Classifier\n",
    "pipeline2 = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(KNeighborsClassifier()))\n",
    "])\n",
    "                 \n",
    "parameters2 = {'clf__estimator__n_neighbors': [1,5],  'clf__estimator__leaf_size': [2,5], 'clf__estimator__p': [1,5]}\n",
    "\n",
    "cv2 = GridSearchCV(pipeline2, param_grid = parameters2, scoring = scorer, verbose = 10)\n",
    "\n",
    "# Find best parameters\n",
    "np.random.seed(41)\n",
    "tuned_model2 = cv2.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task of improving the model is not easy, as the Random Forest classifier is an outstanding algorithm. The choice of the \"KNeighborsClassifier()\" mostly reflected that what was possible. Other algorithms, such as SVC, were very difficult to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([55.0037159 , 19.99312067, 17.07501431, 16.62494526, 14.65011358,\n",
       "        14.59682717, 14.81111932, 14.25886297]),\n",
       " 'std_fit_time': array([41.43601498,  2.28694115,  3.1185102 ,  3.09615973,  0.3363829 ,\n",
       "         0.20105982,  0.83202633,  0.09936335]),\n",
       " 'mean_score_time': array([167.07589002,   0.        , 132.39266534,   0.        ,\n",
       "         97.24220662,   0.        , 104.96142077,   0.        ]),\n",
       " 'std_score_time': array([ 7.62820091,  0.        , 18.4035491 ,  0.        ,  8.17844224,\n",
       "         0.        ,  4.23586402,  0.        ]),\n",
       " 'param_clf__estimator__leaf_size': masked_array(data=[2, 2, 2, 2, 5, 5, 5, 5],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_clf__estimator__n_neighbors': masked_array(data=[1, 1, 5, 5, 1, 1, 5, 5],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_clf__estimator__p': masked_array(data=[1, 5, 1, 5, 1, 5, 1, 5],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'clf__estimator__leaf_size': 2,\n",
       "   'clf__estimator__n_neighbors': 1,\n",
       "   'clf__estimator__p': 1},\n",
       "  {'clf__estimator__leaf_size': 2,\n",
       "   'clf__estimator__n_neighbors': 1,\n",
       "   'clf__estimator__p': 5},\n",
       "  {'clf__estimator__leaf_size': 2,\n",
       "   'clf__estimator__n_neighbors': 5,\n",
       "   'clf__estimator__p': 1},\n",
       "  {'clf__estimator__leaf_size': 2,\n",
       "   'clf__estimator__n_neighbors': 5,\n",
       "   'clf__estimator__p': 5},\n",
       "  {'clf__estimator__leaf_size': 5,\n",
       "   'clf__estimator__n_neighbors': 1,\n",
       "   'clf__estimator__p': 1},\n",
       "  {'clf__estimator__leaf_size': 5,\n",
       "   'clf__estimator__n_neighbors': 1,\n",
       "   'clf__estimator__p': 5},\n",
       "  {'clf__estimator__leaf_size': 5,\n",
       "   'clf__estimator__n_neighbors': 5,\n",
       "   'clf__estimator__p': 1},\n",
       "  {'clf__estimator__leaf_size': 5,\n",
       "   'clf__estimator__n_neighbors': 5,\n",
       "   'clf__estimator__p': 5}],\n",
       " 'split0_test_score': array([nan, nan, nan, nan, nan, nan, nan, nan]),\n",
       " 'split1_test_score': array([nan, nan, nan, nan, nan, nan, nan, nan]),\n",
       " 'split2_test_score': array([nan, nan, nan, nan, nan, nan, nan, nan]),\n",
       " 'split3_test_score': array([nan, nan, nan, nan, nan, nan, nan, nan]),\n",
       " 'split4_test_score': array([nan, nan, nan, nan, nan, nan, nan, nan]),\n",
       " 'mean_test_score': array([nan, nan, nan, nan, nan, nan, nan, nan]),\n",
       " 'std_test_score': array([nan, nan, nan, nan, nan, nan, nan, nan]),\n",
       " 'rank_test_score': array([1, 2, 3, 4, 5, 6, 7, 8])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get results of grid search\n",
    "tuned_model2.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__leaf_size': 2,\n",
       " 'clf__estimator__n_neighbors': 1,\n",
       " 'clf__estimator__p': 1}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters for best mean test score\n",
    "tuned_model2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.135172   0.606956  0.135172  0.214316\n",
      "request                 0.806461   0.751474  0.806461  0.768063\n",
      "offer                   0.995733   0.992393  0.995733  0.994060\n",
      "aid_related             0.574368   0.513044  0.574368  0.480135\n",
      "medical_help            0.913593   0.861243  0.913593  0.885705\n",
      "medical_products        0.940415   0.902019  0.940415  0.919985\n",
      "search_and_rescue       0.972265   0.954869  0.972265  0.962196\n",
      "security                0.978665   0.964935  0.978665  0.971532\n",
      "military                0.967998   0.939927  0.967998  0.953756\n",
      "child_alone             1.000000   1.000000  1.000000  1.000000\n",
      "water                   0.925328   0.884180  0.925328  0.902427\n",
      "food                    0.874581   0.817692  0.874581  0.839016\n",
      "shelter                 0.896068   0.844758  0.896068  0.867021\n",
      "clothing                0.981865   0.968401  0.981865  0.974516\n",
      "money                   0.973331   0.956581  0.973331  0.964442\n",
      "missing_people          0.986589   0.975452  0.986589  0.980989\n",
      "refugees                0.962969   0.935663  0.962969  0.947749\n",
      "death                   0.949253   0.914726  0.949253  0.931000\n",
      "other_aid               0.847607   0.778607  0.847607  0.806687\n",
      "infrastructure_related  0.930357   0.884464  0.930357  0.904863\n",
      "transport               0.949558   0.909878  0.949558  0.929295\n",
      "buildings               0.940415   0.905536  0.940415  0.921114\n",
      "electricity             0.977598   0.958945  0.977598  0.968182\n",
      "tools                   0.991771   0.985719  0.991771  0.988736\n",
      "hospitals               0.990704   0.984294  0.990704  0.986960\n",
      "shops                   0.994514   0.991176  0.994514  0.992842\n",
      "aid_centers             0.985218   0.974537  0.985218  0.979848\n",
      "other_infrastructure    0.953215   0.918604  0.953215  0.935590\n",
      "weather_related         0.710302   0.630800  0.710302  0.630421\n",
      "floods                  0.914813   0.858541  0.914813  0.880548\n",
      "storm                   0.902012   0.851247  0.902012  0.867554\n",
      "fire                    0.988418   0.979970  0.988418  0.984176\n",
      "earthquake              0.900030   0.864238  0.900030  0.874497\n",
      "cold                    0.976989   0.963749  0.976989  0.970021\n",
      "other_weather           0.943767   0.904265  0.943767  0.922766\n",
      "direct_report           0.784669   0.723930  0.784669  0.740155\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for test set\n",
    "tuned_pred_test2 = tuned_model2.predict(X_test)\n",
    "\n",
    "eval_metrics2 = get_eval_metrics(np.array(Y_test), tuned_pred_test2, col_names)\n",
    "\n",
    "print(eval_metrics2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results it becomes clear that the Random Forest classifier tends to have a much better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle best model\n",
    "pickle.dump(tuned_model, open('disaster_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
